{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Basic dataset info ===\n",
            "Shape: (87, 21)\n",
            "Columns: ['gender', 'age', 'university', 'degree_level', 'degree_major', 'academic_year', 'cgpa', 'residential_status', 'campus_discrimination', 'sports_engagement', 'average_sleep', 'study_satisfaction', 'academic_workload ', 'academic_pressure', 'financial_concerns', 'social_relationships', 'depression', 'anxiety', 'isolation', 'future_insecurity', 'stress_relief_activities']\n",
            "\n",
            "Head:\n",
            "   gender  age university   degree_level      degree_major academic_year  \\\n",
            "0    Male   20         PU  Undergraduate      Data Science      2nd year   \n",
            "1    Male   20        UET   Postgraduate  Computer Science      3rd year   \n",
            "2    Male   20       FAST  Undergraduate  Computer Science      3rd year   \n",
            "3    Male   20        UET  Undergraduate  Computer Science      3rd year   \n",
            "4  Female   20        UET  Undergraduate  Computer Science      3rd year   \n",
            "\n",
            "      cgpa residential_status campus_discrimination sports_engagement  ...  \\\n",
            "0  3.0-3.5         Off-Campus                    No         No Sports  ...   \n",
            "1  3.0-3.5         Off-Campus                    No         1-3 times  ...   \n",
            "2  2.5-3.0         Off-Campus                    No         1-3 times  ...   \n",
            "3  2.5-3.0          On-Campus                    No         No Sports  ...   \n",
            "4  3.0-3.5         Off-Campus                   Yes         No Sports  ...   \n",
            "\n",
            "  study_satisfaction  academic_workload   academic_pressure  \\\n",
            "0                  5                   4                  5   \n",
            "1                  5                   4                  4   \n",
            "2                  5                   5                  5   \n",
            "3                  3                   5                  4   \n",
            "4                  3                   5                  5   \n",
            "\n",
            "   financial_concerns  social_relationships  depression  anxiety  isolation  \\\n",
            "0                   4                     3           2        1          1   \n",
            "1                   1                     3           3        3          3   \n",
            "2                   3                     4           2        3          3   \n",
            "3                   4                     1           5        5          5   \n",
            "4                   2                     3           5        5          4   \n",
            "\n",
            "   future_insecurity                           stress_relief_activities  \n",
            "0                  2  Religious Activities, Social Connections, Onli...  \n",
            "1                  4                               Online Entertainment  \n",
            "2                  1  Religious Activities, Sports and Fitness, Onli...  \n",
            "3                  3                               Online Entertainment  \n",
            "4                  4                               Online Entertainment  \n",
            "\n",
            "[5 rows x 21 columns]\n",
            "\n",
            "=== Missing values per column ===\n",
            "gender                      0\n",
            "age                         0\n",
            "university                  0\n",
            "degree_level                0\n",
            "degree_major                0\n",
            "academic_year               0\n",
            "cgpa                        0\n",
            "residential_status          0\n",
            "campus_discrimination       0\n",
            "sports_engagement           0\n",
            "average_sleep               0\n",
            "study_satisfaction          0\n",
            "academic_workload           0\n",
            "academic_pressure           0\n",
            "financial_concerns          0\n",
            "social_relationships        0\n",
            "depression                  0\n",
            "anxiety                     0\n",
            "isolation                   0\n",
            "future_insecurity           0\n",
            "stress_relief_activities    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Cell 1: Imports, config, load data\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Optional libraries (only used if installed)\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "TSNE_AVAILABLE = False\n",
        "UMAP_AVAILABLE = False\n",
        "try:\n",
        "    from openTSNE import TSNE\n",
        "    TSNE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    import umap\n",
        "    UMAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)\n",
        "\n",
        "# -------- Load dataset --------\n",
        "DATA_PATH = \"MentalHealthSurvey.csv\"  # change if your filename is different\n",
        "\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not find {DATA_PATH}. Put the CSV next to this notebook or update DATA_PATH.\"\n",
        "    )\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(\"=== Basic dataset info ===\")\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(\"\\nHead:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n=== Missing values per column ===\")\n",
        "print(df.isna().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b9e6a1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Cell 2: Basic EDA\n",
        "# =========================\n",
        "\n",
        "print(\"=== Descriptive statistics (numeric) ===\")\n",
        "print(df.describe(include=[np.number]))\n",
        "\n",
        "print(\"\\n=== Descriptive statistics (categorical) ===\")\n",
        "print(df.describe(include=[\"object\"]))\n",
        "\n",
        "# Helper plot functions\n",
        "def plot_hist(column, bins=10):\n",
        "    if column not in df.columns:\n",
        "        return\n",
        "    plt.figure()\n",
        "    df[column].dropna().astype(float, errors=\"ignore\").hist(bins=bins)\n",
        "    plt.title(f\"Histogram of {column}\")\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_bar(column):\n",
        "    if column not in df.columns:\n",
        "        return\n",
        "    plt.figure()\n",
        "    df[column].value_counts().plot(kind=\"bar\")\n",
        "    plt.title(f\"Counts of {column}\")\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Choose numeric/ordinal-looking columns from your dataset\n",
        "num_like_cols = [\n",
        "    \"age\", \"cgpa\", \"study_satisfaction\", \"academic_workload\",\n",
        "    \"academic_pressure\", \"financial_concerns\", \"social_relationships\",\n",
        "    \"depression\", \"anxiety\"\n",
        "]\n",
        "for col in num_like_cols:\n",
        "    if col in df.columns:\n",
        "        plot_hist(col)\n",
        "\n",
        "cat_cols = [\n",
        "    \"gender\", \"university\", \"degree_level\", \"residential_status\",\n",
        "    \"sports_engagement\", \"average_sleep\", \"stress_relief_activities\"\n",
        "]\n",
        "for col in cat_cols:\n",
        "    if col in df.columns:\n",
        "        plot_bar(col)\n",
        "\n",
        "# Correlation matrix for numeric columns\n",
        "numeric_cols_raw = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if len(numeric_cols_raw) > 1:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    corr = df[numeric_cols_raw].corr()\n",
        "    im = plt.imshow(corr, interpolation=\"nearest\")\n",
        "    plt.xticks(range(len(numeric_cols_raw)), numeric_cols_raw, rotation=45, ha=\"right\")\n",
        "    plt.yticks(range(len(numeric_cols_raw)), numeric_cols_raw)\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    plt.title(\"Correlation matrix (numeric features)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"\\n=== Correlation matrix ===\")\n",
        "    print(corr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f93fb59",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Cell 2.5: Data cleaning & quality checks\n",
        "# =========================\n",
        "\n",
        "import re\n",
        "\n",
        "print(\"Shape BEFORE cleaning:\", df.shape)\n",
        "\n",
        "# 1) Strip whitespace from string columns\n",
        "obj_cols = df.select_dtypes(include=\"object\").columns\n",
        "for col in obj_cols:\n",
        "    df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "# 2) Check and remove duplicate rows\n",
        "dup_mask = df.duplicated()\n",
        "print(\"Number of duplicate rows:\", dup_mask.sum())\n",
        "\n",
        "if dup_mask.sum() > 0:\n",
        "    df = df[~dup_mask].copy()\n",
        "    print(\"Duplicates removed.\")\n",
        "else:\n",
        "    print(\"No duplicate rows found.\")\n",
        "\n",
        "print(\"Shape AFTER removing duplicates:\", df.shape)\n",
        "\n",
        "# 3) Optional: normalize some categorical text (just stripping, not forcing lowercase globally)\n",
        "# (You can add more columns here if needed)\n",
        "clean_cats = [\n",
        "    \"gender\",\n",
        "    \"university\",\n",
        "    \"degree_level\",\n",
        "    \"residential_status\",\n",
        "    \"sports_engagement\",\n",
        "    \"average_sleep\",\n",
        "    \"campus_discrimination\",\n",
        "]\n",
        "for col in clean_cats:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].str.strip()\n",
        "\n",
        "# 4) Convert CGPA ranges (e.g., \"3.0-3.5\") into numeric midpoint\n",
        "def cgpa_to_numeric(val):\n",
        "    if pd.isna(val):\n",
        "        return np.nan\n",
        "    if isinstance(val, (int, float)):\n",
        "        return float(val)\n",
        "\n",
        "    s = str(val)\n",
        "    # Case like \"3.0-3.5\"\n",
        "    if \"-\" in s or \"–\" in s:\n",
        "        parts = re.split(r\"[-–]\", s)\n",
        "        nums = []\n",
        "        for p in parts:\n",
        "            p = p.strip()\n",
        "            if not p:\n",
        "                continue\n",
        "            try:\n",
        "                nums.append(float(p))\n",
        "            except ValueError:\n",
        "                pass\n",
        "        if len(nums) == 2:\n",
        "            return (nums[0] + nums[1]) / 2.0\n",
        "\n",
        "    # Fallback: extract number like \"3.2\" or \"3.2+\"\n",
        "    s_clean = re.sub(r\"[^0-9.]\", \"\", s)\n",
        "    try:\n",
        "        return float(s_clean)\n",
        "    except ValueError:\n",
        "        return np.nan\n",
        "\n",
        "if \"cgpa\" in df.columns:\n",
        "    df[\"cgpa\"] = df[\"cgpa\"].apply(cgpa_to_numeric)\n",
        "\n",
        "# 5) Convert average_sleep (e.g., \"4-6 hrs\") -> numeric hours\n",
        "def sleep_to_hours(val):\n",
        "    if pd.isna(val):\n",
        "        return np.nan\n",
        "    s = str(val).lower()\n",
        "\n",
        "    # Case like \"4-6 hrs\"\n",
        "    if \"-\" in s or \"–\" in s:\n",
        "        parts = re.split(r\"[-–]\", s)\n",
        "        nums = []\n",
        "        for p in parts:\n",
        "            p = re.sub(r\"[^0-9.]\", \"\", p)\n",
        "            if p:\n",
        "                try:\n",
        "                    nums.append(float(p))\n",
        "                except ValueError:\n",
        "                    pass\n",
        "        if len(nums) == 2:\n",
        "            return (nums[0] + nums[1]) / 2.0\n",
        "\n",
        "    # Some custom phrases (you can tweak these if your dataset has them)\n",
        "    if \"less\" in s and \"4\" in s:\n",
        "        return 3.0\n",
        "    if \"more\" in s and \"9\" in s:\n",
        "        return 9.0\n",
        "\n",
        "    # Fallback: pull any number\n",
        "    s_clean = re.sub(r\"[^0-9.]\", \"\", s)\n",
        "    if s_clean:\n",
        "        try:\n",
        "            return float(s_clean)\n",
        "        except ValueError:\n",
        "            return np.nan\n",
        "\n",
        "    return np.nan\n",
        "\n",
        "if \"average_sleep\" in df.columns:\n",
        "    df[\"average_sleep_hours\"] = df[\"average_sleep\"].apply(sleep_to_hours)\n",
        "\n",
        "# 6) Show basic value-counts for key categorical columns (for EDA & label checking)\n",
        "print(\"\\n=== Value counts for key categorical columns ===\")\n",
        "for col in [\"gender\", \"university\", \"degree_level\", \"residential_status\",\n",
        "            \"sports_engagement\", \"average_sleep\"]:\n",
        "    if col in df.columns:\n",
        "        print(f\"\\nColumn: {col}\")\n",
        "        print(df[col].value_counts())\n",
        "\n",
        "# 7) Missing values BEFORE imputation\n",
        "print(\"\\nMissing values BEFORE imputation:\")\n",
        "print(df.isna().sum())\n",
        "\n",
        "# 8) Simple missing-value handling\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
        "\n",
        "# For numeric: fill with median\n",
        "for col in num_cols:\n",
        "    median_val = df[col].median()\n",
        "    df[col].fillna(median_val, inplace=True)\n",
        "\n",
        "# For categorical: fill with mode, or \"Unknown\" if no mode\n",
        "for col in cat_cols:\n",
        "    mode = df[col].mode()\n",
        "    if not mode.empty:\n",
        "        df[col].fillna(mode[0], inplace=True)\n",
        "    else:\n",
        "        df[col].fillna(\"Unknown\", inplace=True)\n",
        "\n",
        "print(\"\\nMissing values AFTER imputation:\")\n",
        "print(df.isna().sum())\n",
        "\n",
        "print(\"\\nShape AFTER cleaning + imputation:\", df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c14778a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Cell 3: Target & preprocessing (no sklearn)\n",
        "# =========================\n",
        "\n",
        "TARGET_COL = \"depression\"\n",
        "\n",
        "if TARGET_COL not in df.columns:\n",
        "    raise KeyError(f\"Target column '{TARGET_COL}' not found in dataset.\")\n",
        "\n",
        "# Drop rows with missing target\n",
        "df = df.dropna(subset=[TARGET_COL]).copy()\n",
        "\n",
        "# Binary target: 0 = low/moderate (<=2) ; 1 = high (>=3)\n",
        "def make_binary_target(x):\n",
        "    try:\n",
        "        x_float = float(x)\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "    return 1 if x_float >= 3 else 0\n",
        "\n",
        "y = df[TARGET_COL].apply(make_binary_target)\n",
        "df = df[~y.isna()].copy()\n",
        "y = y[~y.isna()].astype(int).values  # numpy array\n",
        "\n",
        "print(\"Target value counts (0=low/moderate, 1=high):\")\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "for u, c in zip(unique, counts):\n",
        "    print(f\"{u}: {c}\")\n",
        "\n",
        "# Features\n",
        "X = df.drop(columns=[TARGET_COL])\n",
        "\n",
        "# numeric vs categorical\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = [c for c in X.columns if c not in numeric_features]\n",
        "\n",
        "print(\"\\nNumeric features:\", numeric_features)\n",
        "print(\"Categorical features:\", categorical_features)\n",
        "\n",
        "# One-hot encode categoricals using pandas\n",
        "X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=False)\n",
        "\n",
        "# Standardize all columns\n",
        "X_values = X_encoded.values.astype(float)\n",
        "feature_means = X_values.mean(axis=0)\n",
        "feature_stds = X_values.std(axis=0) + 1e-6\n",
        "X_scaled = (X_values - feature_means) / feature_stds\n",
        "\n",
        "feature_names = X_encoded.columns.tolist()\n",
        "print(\"\\nEncoded feature matrix shape:\", X_scaled.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4afb8671",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Cell 4: Train/test split & metric functions\n",
        "# =========================\n",
        "\n",
        "def train_test_split_manual(X, y, test_size=0.2, random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "    n_samples = X.shape[0]\n",
        "    indices = np.random.permutation(n_samples)\n",
        "    test_count = int(n_samples * test_size)\n",
        "    test_idx = indices[:test_count]\n",
        "    train_idx = indices[test_count:]\n",
        "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split_manual(\n",
        "    X_scaled, y, test_size=0.2, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
        "\n",
        "# ---- metric functions (FIXED SHAPE ISSUE) ----\n",
        "def classification_metrics(y_true, y_pred):\n",
        "    # Convert everything to 1D int arrays\n",
        "    y_true = np.array(y_true).astype(int).reshape(-1)\n",
        "    y_pred = np.array(y_pred).astype(int).reshape(-1)\n",
        "\n",
        "    assert y_true.shape == y_pred.shape\n",
        "\n",
        "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n",
        "    precision = tp / (tp + fp + 1e-8) if (tp + fp) > 0 else 0.0\n",
        "    recall = tp / (tp + fn + 1e-8) if (tp + fn) > 0 else 0.0\n",
        "    f1 = (2 * precision * recall / (precision + recall + 1e-8)\n",
        "          if (precision + recall) > 0 else 0.0)\n",
        "\n",
        "    cm = np.array([[tn, fp],\n",
        "                   [fn, tp]])\n",
        "    return accuracy, precision, recall, f1, cm\n",
        "\n",
        "def plot_confusion_matrix(cm, title=\"Confusion Matrix\"):\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation=\"nearest\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
        "    plt.colorbar()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Convert to torch tensors for models\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "y_test_t  = torch.tensor(y_test,  dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "input_dim = X_train.shape[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c146f9bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Cell 4: Train/test split & metric functions\n",
        "# =========================\n",
        "\n",
        "def train_test_split_manual(X, y, test_size=0.2, random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "    n_samples = X.shape[0]\n",
        "    indices = np.random.permutation(n_samples)\n",
        "    test_count = int(n_samples * test_size)\n",
        "    test_idx = indices[:test_count]\n",
        "    train_idx = indices[test_count:]\n",
        "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split_manual(\n",
        "    X_scaled, y, test_size=0.2, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
        "\n",
        "# ---- metric functions (FIXED SHAPE ISSUE) ----\n",
        "def classification_metrics(y_true, y_pred):\n",
        "    # Convert everything to 1D int arrays\n",
        "    y_true = np.array(y_true).astype(int).reshape(-1)\n",
        "    y_pred = np.array(y_pred).astype(int).reshape(-1)\n",
        "\n",
        "    assert y_true.shape == y_pred.shape\n",
        "\n",
        "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n",
        "    precision = tp / (tp + fp + 1e-8) if (tp + fp) > 0 else 0.0\n",
        "    recall = tp / (tp + fn + 1e-8) if (tp + fn) > 0 else 0.0\n",
        "    f1 = (2 * precision * recall / (precision + recall + 1e-8)\n",
        "          if (precision + recall) > 0 else 0.0)\n",
        "\n",
        "    cm = np.array([[tn, fp],\n",
        "                   [fn, tp]])\n",
        "    return accuracy, precision, recall, f1, cm\n",
        "\n",
        "def plot_confusion_matrix(cm, title=\"Confusion Matrix\"):\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation=\"nearest\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
        "    plt.colorbar()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Convert to torch tensors for models\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "y_test_t  = torch.tensor(y_test,  dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "input_dim = X_train.shape[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62f97fc2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Cell 5: Logistic Regression (PyTorch)\n",
        "# =========================\n",
        "\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)  # logits\n",
        "\n",
        "def train_model(model, X_train, y_train, X_val, y_val,\n",
        "                lr=1e-3, epochs=500, batch_size=16, weight_decay=0.0,\n",
        "                verbose=False, model_name=\"model\"):\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    n_samples = X_train.shape[0]\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        perm = torch.randperm(n_samples)\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            idx = perm[i:i+batch_size]\n",
        "            batch_x = X_train[idx]\n",
        "            batch_y = y_train[idx]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch_x)\n",
        "            loss = criterion(logits, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # validation (using test set as val)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_logits = model(X_val)\n",
        "            val_probs = torch.sigmoid(val_logits).cpu().numpy().flatten()\n",
        "            val_pred  = (val_probs >= 0.5).astype(int)\n",
        "            _, _, _, f1, _ = classification_metrics(y_val, val_pred)\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "        if verbose and (epoch + 1) % 50 == 0:\n",
        "            print(f\"{model_name} Epoch {epoch+1}/{epochs}, best F1 so far: {best_f1:.4f}\")\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return model\n",
        "\n",
        "log_reg_model = LogisticRegressionModel(input_dim)\n",
        "log_reg_model = train_model(\n",
        "    log_reg_model,\n",
        "    X_train_t, y_train_t,\n",
        "    X_test_t,  y_test_t,\n",
        "    lr=1e-3,\n",
        "    epochs=300,\n",
        "    batch_size=8,\n",
        "    weight_decay=1e-4,\n",
        "    verbose=True,\n",
        "    model_name=\"LogisticRegression\"\n",
        ")\n",
        "\n",
        "# evaluation\n",
        "log_reg_model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = log_reg_model(X_test_t)\n",
        "    probs  = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "    y_pred_lr = (probs >= 0.5).astype(int)\n",
        "\n",
        "acc_lr, prec_lr, rec_lr, f1_lr, cm_lr = classification_metrics(y_test, y_pred_lr)\n",
        "\n",
        "print(\"\\n=== Logistic Regression performance ===\")\n",
        "print(\"Accuracy :\", acc_lr)\n",
        "print(\"Precision:\", prec_lr)\n",
        "print(\"Recall   :\", rec_lr)\n",
        "print(\"F1-score :\", f1_lr)\n",
        "print(\"Confusion matrix:\\n\", cm_lr)\n",
        "\n",
        "plot_confusion_matrix(cm_lr, title=\"Logistic Regression - Confusion Matrix\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0280d0e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Cell 6: Neural Network (MLP)\n",
        "# =========================\n",
        "\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=(64, 32), dropout=0.1):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev = input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers.append(nn.Linear(prev, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            if dropout > 0:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            prev = h\n",
        "        layers.append(nn.Linear(prev, 1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "mlp_model = MLPClassifier(input_dim=input_dim, hidden_dims=(64, 32), dropout=0.1)\n",
        "\n",
        "mlp_model = train_model(\n",
        "    mlp_model,\n",
        "    X_train_t, y_train_t,\n",
        "    X_test_t,  y_test_t,\n",
        "    lr=1e-3,\n",
        "    epochs=500,\n",
        "    batch_size=8,\n",
        "    weight_decay=1e-4,\n",
        "    verbose=True,\n",
        "    model_name=\"NeuralNetwork\"\n",
        ")\n",
        "\n",
        "mlp_model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = mlp_model(X_test_t)\n",
        "    probs  = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "    y_pred_mlp = (probs >= 0.5).astype(int)\n",
        "\n",
        "acc_mlp, prec_mlp, rec_mlp, f1_mlp, cm_mlp = classification_metrics(y_test, y_pred_mlp)\n",
        "\n",
        "print(\"\\n=== Neural Network (MLP) performance ===\")\n",
        "print(\"Accuracy :\", acc_mlp)\n",
        "print(\"Precision:\", prec_mlp)\n",
        "print(\"Recall   :\", rec_mlp)\n",
        "print(\"F1-score :\", f1_mlp)\n",
        "print(\"Confusion matrix:\\n\", cm_mlp)\n",
        "\n",
        "plot_confusion_matrix(cm_mlp, title=\"Neural Network (MLP) - Confusion Matrix\")\n",
        "\n",
        "print(\"\\n=== Model comparison ===\")\n",
        "print(f\"LogReg -> acc={acc_lr:.4f}, prec={prec_lr:.4f}, rec={rec_lr:.4f}, f1={f1_lr:.4f}\")\n",
        "print(f\"MLP    -> acc={acc_mlp:.4f}, prec={prec_mlp:.4f}, rec={rec_mlp:.4f}, f1={f1_mlp:.4f}\")\n",
        "best_model = \"MLP\" if f1_mlp >= f1_lr else \"Logistic Regression\"\n",
        "print(\"Best model based on F1:\", best_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c94463b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Cell 6.5: Model Comparison + Error Statistics + Deep Analysis\n",
        "# =========================\n",
        "\n",
        "print(\"\\n====================\")\n",
        "print(\"DETAILED MODEL ANALYSIS\")\n",
        "print(\"====================\\n\")\n",
        "\n",
        "# 1) Comparison Table\n",
        "comparison = pd.DataFrame({\n",
        "    \"Model\": [\"Logistic Regression\", \"Neural Network (MLP)\"],\n",
        "    \"Accuracy\": [acc_lr, acc_mlp],\n",
        "    \"Precision\": [prec_lr, prec_mlp],\n",
        "    \"Recall\": [rec_lr, rec_mlp],\n",
        "    \"F1-score\": [f1_lr, f1_mlp],\n",
        "})\n",
        "\n",
        "print(\"=== Model Performance Comparison ===\")\n",
        "print(comparison.to_string(index=False))\n",
        "\n",
        "# Identify best and worst model\n",
        "if f1_mlp > f1_lr:\n",
        "    best_model = \"Neural Network (MLP)\"\n",
        "    worst_model = \"Logistic Regression\"\n",
        "else:\n",
        "    best_model = \"Logistic Regression\"\n",
        "    worst_model = \"Neural Network (MLP)\"\n",
        "\n",
        "print(f\"\\nBest Model Based on F1-score: {best_model}\")\n",
        "print(f\"Worst Model Based on F1-score: {worst_model}\")\n",
        "\n",
        "# 2) Error statistics for classification\n",
        "# Convert predictions to numeric error indicator:\n",
        "#   error = |true - predicted|\n",
        "# This gives:\n",
        "#   correct prediction → 0\n",
        "#   wrong prediction   → 1\n",
        "\n",
        "lr_errors = np.abs(y_test - y_pred_lr)\n",
        "mlp_errors = np.abs(y_test - y_pred_mlp)\n",
        "\n",
        "print(\"\\n=== Error Statistics (Classification Equivalent) ===\")\n",
        "print(\"Logistic Regression Error Rate:\", lr_errors.mean())\n",
        "print(\"Neural Network Error Rate:\", mlp_errors.mean())\n",
        "\n",
        "print(\"\\nStandard Deviation of Errors:\")\n",
        "print(\"Logistic Regression STD:\", lr_errors.std())\n",
        "print(\"Neural Network STD:\", mlp_errors.std())\n",
        "\n",
        "# 3) Confusion matrices already plotted earlier, but print numeric versions again\n",
        "print(\"\\n=== Confusion Matrices ===\")\n",
        "print(\"Logistic Regression Confusion Matrix:\\n\", cm_lr)\n",
        "print(\"Neural Network Confusion Matrix:\\n\", cm_mlp)\n",
        "\n",
        "# 4) Additional Insight: False Positives & False Negatives\n",
        "def analyze_confusion(cm, name):\n",
        "    tn, fp = cm[0]\n",
        "    fn, tp = cm[1]\n",
        "    print(f\"\\n{name} Error Breakdown:\")\n",
        "    print(f\"True Positives:  {tp}\")\n",
        "    print(f\"True Negatives:  {tn}\")\n",
        "    print(f\"False Positives: {fp}  (predict high depression but actually low)\")\n",
        "    print(f\"False Negatives: {fn}  (predict low depression but actually high)\")\n",
        "\n",
        "analyze_confusion(cm_lr, \"Logistic Regression\")\n",
        "analyze_confusion(cm_mlp, \"Neural Network\")\n",
        "\n",
        "print(\"\\n=== Interpretation Summary ===\")\n",
        "if best_model == \"Neural Network (MLP)\":\n",
        "    print(\"- The MLP generalizes better with higher F1-score.\")\n",
        "    print(\"- Logistic Regression is simpler but struggles with class boundaries.\")\n",
        "else:\n",
        "    print(\"- Logistic Regression performs slightly better in this dataset.\")\n",
        "    print(\"- The NN may be overfitting due to small dataset size.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ededc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Cell X: Extra model comparison using scikit-learn\n",
        "# =========================\n",
        "\n",
        "# If sklearn is not installed:\n",
        "#   pip install scikit-learn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "# Use the same features and labels as before: X_scaled, y\n",
        "# (Re-splitting here with stratification for fair comparison)\n",
        "X_train_skl, X_test_skl, y_train_skl, y_test_skl = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "# Define sklearn models\n",
        "sk_models = {\n",
        "    \"LogReg (sklearn)\": LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
        "    \"RandomForest\": RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=None,\n",
        "        random_state=RANDOM_STATE,\n",
        "        class_weight=\"balanced\"\n",
        "    ),\n",
        "    \"SVM (RBF)\": SVC(\n",
        "        kernel=\"rbf\",\n",
        "        probability=True,\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=RANDOM_STATE\n",
        "    ),\n",
        "    \"KNN (k=5)\": KNeighborsClassifier(n_neighbors=5)\n",
        "}\n",
        "\n",
        "results = []\n",
        "cms_skl = {}\n",
        "\n",
        "for name, model in sk_models.items():\n",
        "    model.fit(X_train_skl, y_train_skl)\n",
        "    y_pred = model.predict(X_test_skl)\n",
        "\n",
        "    acc = accuracy_score(y_test_skl, y_pred)\n",
        "    prec = precision_score(y_test_skl, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test_skl, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test_skl, y_pred, zero_division=0)\n",
        "\n",
        "    cm = confusion_matrix(y_test_skl, y_pred)\n",
        "    cms_skl[name] = cm\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Precision\": prec,\n",
        "        \"Recall\": rec,\n",
        "        \"F1-score\": f1\n",
        "    })\n",
        "\n",
        "# ---- Show table of metrics ----\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"=== Sklearn Model Performance Comparison ===\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# ---- Bar chart of F1-scores ----\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(results_df[\"Model\"], results_df[\"F1-score\"])\n",
        "plt.title(\"F1-score comparison (sklearn models)\")\n",
        "plt.ylabel(\"F1-score\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---- Confusion matrices for each model ----\n",
        "for name, cm in cms_skl.items():\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation=\"nearest\")\n",
        "    plt.title(f\"{name} - Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
        "    plt.colorbar()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Identify best sklearn model by F1\n",
        "best_row = results_df.iloc[results_df[\"F1-score\"].idxmax()]\n",
        "print(\"\\nBest sklearn model based on F1-score:\")\n",
        "print(best_row)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "436327a3",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "711b8b7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Cell 7: Dimensionality reduction (PCA, Random Projection, t-SNE/UMAP)\n",
        "# =========================\n",
        "\n",
        "# ----- PCA from scratch -----\n",
        "def pca_from_scratch(X, n_components=2):\n",
        "    # X should be (n_samples, n_features)\n",
        "    Xc = X - X.mean(axis=0)\n",
        "    cov = np.dot(Xc.T, Xc) / (Xc.shape[0] - 1)\n",
        "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
        "    idx = np.argsort(eigvals)[::-1]\n",
        "    eigvals = eigvals[idx]\n",
        "    eigvecs = eigvecs[:, idx]\n",
        "    comps = eigvecs[:, :n_components]\n",
        "    X_pca = np.dot(Xc, comps)\n",
        "    var_ratio = eigvals[:n_components] / eigvals.sum()\n",
        "    return X_pca, var_ratio\n",
        "\n",
        "# Run PCA\n",
        "X_pca, pca_var = pca_from_scratch(X_scaled, n_components=2)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, alpha=0.8)\n",
        "plt.title(\"PCA (2 components)\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"PCA explained variance ratio:\", pca_var)\n",
        "\n",
        "\n",
        "# ----- Random Projection (simple linear DR) -----\n",
        "def random_projection(X, n_components=2, random_state=42):\n",
        "    \"\"\"\n",
        "    Simple random linear projection:\n",
        "    X_proj = X @ W, where W has random normal entries.\n",
        "    Used as a very simple baseline for dimensionality reduction.\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    W = rng.normal(size=(X.shape[1], n_components))  # (n_features, n_components)\n",
        "    X_rp = np.dot(X, W)\n",
        "    return X_rp\n",
        "\n",
        "X_rp = random_projection(X_scaled, n_components=2, random_state=RANDOM_STATE)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X_rp[:, 0], X_rp[:, 1], c=y, alpha=0.8)\n",
        "plt.title(\"Random Projection (2D)\")\n",
        "plt.xlabel(\"RP1\")\n",
        "plt.ylabel(\"RP2\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ----- t-SNE or UMAP (if installed) -----\n",
        "if TSNE_AVAILABLE:\n",
        "    print(\"Running t-SNE (openTSNE)...\")\n",
        "    tsne = TSNE(\n",
        "        n_components=2,\n",
        "        learning_rate=\"auto\",\n",
        "        initialization=\"random\",\n",
        "        random_state=RANDOM_STATE,\n",
        "        perplexity=min(30, max(5, X_scaled.shape[0] // 3))\n",
        "    )\n",
        "    X_tsne = np.asarray(tsne.fit(X_scaled))\n",
        "    plt.figure()\n",
        "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, alpha=0.8)\n",
        "    plt.title(\"t-SNE (2D)\")\n",
        "    plt.xlabel(\"Dim 1\")\n",
        "    plt.ylabel(\"Dim 2\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "elif UMAP_AVAILABLE:\n",
        "    print(\"Running UMAP...\")\n",
        "    reducer = umap.UMAP(\n",
        "        n_components=2,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_neighbors=10,\n",
        "        min_dist=0.1\n",
        "    )\n",
        "    X_umap = reducer.fit_transform(X_scaled)\n",
        "    plt.figure()\n",
        "    plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, alpha=0.8)\n",
        "    plt.title(\"UMAP (2D)\")\n",
        "    plt.xlabel(\"Dim 1\")\n",
        "    plt.ylabel(\"Dim 2\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"No openTSNE or UMAP installed. Install one for non-linear DR:\")\n",
        "    print(\"  pip install openTSNE\")\n",
        "    print(\"or\")\n",
        "    print(\"  pip install umap-learn\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e80cd6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Cell 8: Feature importance (XGBoost + SHAP)\n",
        "# =========================\n",
        "\n",
        "if XGBOOST_AVAILABLE:\n",
        "    print(\"Training XGBoost classifier for feature importance...\")\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        max_depth=4,\n",
        "        learning_rate=0.1,\n",
        "        n_estimators=200,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.9,\n",
        "        reg_lambda=1.0,\n",
        "        objective=\"binary:logistic\",\n",
        "        random_state=RANDOM_STATE,\n",
        "        eval_metric=\"logloss\"\n",
        "    )\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "\n",
        "    importances = xgb_model.feature_importances_\n",
        "    sorted_idx = np.argsort(importances)[::-1]\n",
        "    top_k = min(15, len(feature_names))\n",
        "    top_idx = sorted_idx[:top_k]\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.barh(range(len(top_idx)), importances[top_idx][::-1])\n",
        "    plt.yticks(range(len(top_idx)), [feature_names[i] for i in top_idx][::-1])\n",
        "    plt.title(\"XGBoost Feature Importances (Top 15)\")\n",
        "    plt.xlabel(\"Importance\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nTop features by importance:\")\n",
        "    for i in top_idx:\n",
        "        print(f\"{feature_names[i]}: {importances[i]:.4f}\")\n",
        "\n",
        "    if SHAP_AVAILABLE:\n",
        "        print(\"\\nRunning SHAP TreeExplainer...\")\n",
        "        explainer = shap.TreeExplainer(xgb_model)\n",
        "        shap_values = explainer.shap_values(X_train)\n",
        "        shap.summary_plot(shap_values, X_train, feature_names=feature_names, show=True)\n",
        "    else:\n",
        "        print(\"SHAP not installed. Install with: pip install shap\")\n",
        "else:\n",
        "    print(\"XGBoost not installed. Install with: pip install xgboost\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
